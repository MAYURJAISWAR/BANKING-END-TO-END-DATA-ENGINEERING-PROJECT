1. . Created the required table structures in Snowflake AWS_SOURCE_DATA_SCHEMA to hold data fetched from the S3 bucket

CREATE TABLE district_tbl(
a1 INT PRIMARY KEY,
a2 VARCHAR(100),
a3 VARCHAR(100),
a4 INT,
a5 INT,
a6 INT,
a7 INT,
a8 INT,	
a9 INT,
a10	FLOAT,
a11 INT,
a12 FLOAT,
a13 FLOAT,
a14 INT,
a15	INT,
a16 INT
);


CREATE TABLE account_tbl(
account_id INT PRIMARY KEY,
district_id	INT,
frequency	VARCHAR(40),
date INT,
account_type VARCHAR(100) 
); 


CREATE TABLE order_tbl (
order_id	INT PRIMARY KEY,
account_id	INT,
bank_to	VARCHAR(45),
account_to	INT,
amount FLOAT
);

CREATE TABLE loan_tbl(
loan_id	INT ,
account_id	INT,
date INT,
amount	INT,
duration	INT,
payment	INT,
status VARCHAR(35)
);


CREATE TABLE transaction_tbl(
trans_id INT,	
account_id	INT,
date	DATE,
type	VARCHAR(30),
operation	VARCHAR(40),
amount	INT,
balance	FLOAT,
purpose	VARCHAR(40),
bank	VARCHAR(45),
account_partner_id INT
);


2. Created a storage integration in Snowflake to connect to the AWS S3 bucket

CREATE STORAGE INTEGRATION aws_s3_integration 
TYPE=external_stage
STORAGE_PROVIDER=s3
enabled=TRUE
STORAGE_AWS_ROLE_ARN='arn:aws:iam::183295428453:role/bank_data_access_role'
STORAGE_ALLOWED_LOCATIONS=('S3://czechoslovakia-banking-data/');

3.  Verified whether the storage integration was successfully created

DESC INTEGRATION aws_s3_integration;

4. Created a CSV file format for staging, as the source files were in CSV format

5. Created an external stage to land raw data in a staging location in Snowflake

6.  Verified whether the stage was created properly

7. Created Snowpipes for auto-ingesting data from S3 to the corresponding Snowflake tables

8. Triggered manual refresh of a pipe to check whether data had reached Snowflake

9. Verified the number of records loaded into the table

